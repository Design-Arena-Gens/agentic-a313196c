{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dromen NLP Pipeline (Colab)\n\n",
        "Voer deze notebook in volgorde uit:\n",
        "- Installeer libs en modellen\n",
        "- Laad/collecteer dromen (Reddit of fallback-bestand)\n",
        "- Preprocess + Sentiment + LDA (coherence-optimalisatie)\n",
        "- Exporteer CSV (`outputs/dreams_results.csv`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "install"},
      "source": [
        "!pip -q install spacy==3.7.4 nltk==3.9.1 gensim==4.3.2 pandas==2.2.3 numpy==1.26.4 scikit-learn==1.5.2 vaderSentiment==3.3.2 requests==2.32.3 tqdm==4.66.5\n",
        "import nltk, spacy\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "import sys, subprocess\n",
        "try:\n",
        "    spacy.load('en_core_web_sm')\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'spacy', 'download', 'en_core_web_sm'], check=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "pipeline"},
      "source": [
        "import argparse\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Iterable, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "import spacy\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "\n",
        "SAMPLE = [\n",
        "    'I was flying over a vast city at night, feeling free and weightless.',\n",
        "    'I dreamed of being chased through a forest by a shadowy figure.',\n",
        "    'I found myself taking an exam I had not studied for, panicking.',\n",
        "    'I met an old friend on a train that never seemed to stop.',\n",
        "    'I was underwater breathing easily, watching colorful fish swim by.',\n",
        "    'I kept losing my teeth one by one and felt embarrassed.',\n",
        "    'I discovered a hidden room in my childhood home, full of light.',\n",
        "    'I stood on a stage and forgot all my lines.',\n",
        "    'I was late for a flight and the airport kept changing.',\n",
        "    'I drove a car with no brakes down a winding hill.'\n",
        "] * 6\n",
        "\n",
        "@dataclass\n",
        "class DreamRecord:\n",
        "    id: str\n",
        "    raw_text: str\n",
        "    clean_text: str\n",
        "    emotion_score: float\n",
        "    topic_id: int\n",
        "    topic_keywords: str\n",
        "\n",
        "def collect_from_reddit(limit: int = 500, user_agent: str = 'dreams-nlp/1.0') -> List[str]:\n",
        "    collected: List[str] = []\n",
        "    after = None\n",
        "    per_page = min(100, max(1, limit))\n",
        "    headers = {'User-Agent': user_agent}\n",
        "    while len(collected) < limit:\n",
        "        url = f'https://www.reddit.com/r/Dreams/top.json?limit={per_page}&t=year'\n",
        "        if after:\n",
        "            url += f'&after={after}'\n",
        "        try:\n",
        "            r = requests.get(url, headers=headers, timeout=15)\n",
        "            if r.status_code != 200:\n",
        "                break\n",
        "            data = r.json()\n",
        "            children = data.get('data', {}).get('children', [])\n",
        "            if not children:\n",
        "                break\n",
        "            for ch in children:\n",
        "                post = ch.get('data', {})\n",
        "                title = post.get('title') or ''\n",
        "                selftext = post.get('selftext') or ''\n",
        "                text = (title + '. ' + selftext).strip()\n",
        "                if len(text.split()) >= 5:\n",
        "                    collected.append(text)\n",
        "                if len(collected) >= limit:\n",
        "                    break\n",
        "            after = data.get('data', {}).get('after')\n",
        "            if not after:\n",
        "                break\n",
        "        except Exception:\n",
        "            break\n",
        "    return collected\n",
        "\n",
        "def clean_texts(texts: Iterable[str], nlp, stop_words: set) -> List[List[str]]:\n",
        "    cleaned = []\n",
        "    import re\n",
        "    pattern = re.compile(r'[^a-z\\s]')\n",
        "    for doc in nlp.pipe((t.lower() for t in texts), batch_size=64):\n",
        "        text = doc.text\n",
        "        text = pattern.sub(' ', text)\n",
        "        toks = []\n",
        "        for tok in doc:\n",
        "            if tok.is_space or tok.is_punct:\n",
        "                continue\n",
        "            lemma = tok.lemma_.strip().lower()\n",
        "            if not lemma or lemma in stop_words or len(lemma) < 2:\n",
        "                continue\n",
        "            if lemma.isnumeric():\n",
        "                continue\n",
        "            toks.append(lemma)\n",
        "        cleaned.append(toks)\n",
        "    return cleaned\n",
        "\n",
        "def compute_sentiment(texts: List[str]) -> List[float]:\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    return [float(sia.polarity_scores(t).get('compound', 0.0)) for t in texts]\n",
        "\n",
        "def train_lda_and_choose_k(tokenized: List[List[str]], k_min: int = 5, k_max: int = 15, random_state: int = 42):\n",
        "    dictionary = corpora.Dictionary(tokenized)\n",
        "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "    corpus = [dictionary.doc2bow(toks) for toks in tokenized]\n",
        "    best_model, best_k, best_coh = None, None, -1.0\n",
        "    for k in range(k_min, k_max + 1):\n",
        "        if k <= 1: continue\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=random_state, passes=5, iterations=200, chunksize=2000)\n",
        "        cm = CoherenceModel(model=model, texts=tokenized, dictionary=dictionary, coherence='c_v')\n",
        "        coh = float(cm.get_coherence())\n",
        "        if coh > best_coh:\n",
        "            best_model, best_k, best_coh = model, k, coh\n",
        "    corpus = [dictionary.doc2bow(toks) for toks in tokenized]\n",
        "    return best_model, dictionary, corpus\n",
        "\n",
        "def topic_for_doc(model, bow):\n",
        "    dist = model.get_document_topics(bow, minimum_probability=0.0)\n",
        "    if not dist: return 0, 0.0\n",
        "    tid, prob = max(dist, key=lambda x: x[1])\n",
        "    return int(tid), float(prob)\n",
        "\n",
        "def top_keywords_for_topic(model, topic_id: int, topn: int = 8) -> str:\n",
        "    terms = model.show_topic(topic_id, topn=topn)\n",
        "    return ', '.join([w for w, _ in terms])\n",
        "\n",
        "def run_pipeline(out_csv: str, source: str = 'reddit', limit: int = 500, k_min: int = 5, k_max: int = 15):\n",
        "    if source == 'reddit':\n",
        "        texts = collect_from_reddit(limit=limit)\n",
        "        if len(texts) < max(50, int(0.6 * limit)):\n",
        "            texts.extend(SAMPLE)\n",
        "    else:\n",
        "        texts = SAMPLE\n",
        "    texts = texts[:limit]\n",
        "    nlp = spacy.load('en_core_web_sm', disable=['ner','parser'])\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokenized = clean_texts(texts, nlp, stop_words)\n",
        "    sentiments = compute_sentiment(texts)\n",
        "    model, dictionary, corpus = train_lda_and_choose_k(tokenized, k_min=k_min, k_max=k_max)\n",
        "    os.makedirs(os.path.dirname(out_csv) or '.', exist_ok=True)\n",
        "    with open(out_csv, 'w', encoding='utf-8', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['id','raw_text','clean_text','emotion_score','topic_id','topic_keywords'])\n",
        "        writer.writeheader()\n",
        "        for i, (raw, toks, bow, emo) in enumerate(zip(texts, tokenized, corpus, sentiments), start=1):\n",
        "            t_id, _ = topic_for_doc(model, bow)\n",
        "            kw = top_keywords_for_topic(model, t_id)\n",
        "            writer.writerow({\n",
        "                'id': str(i),\n",
        "                'raw_text': raw,\n",
        "                'clean_text': ' '.join(toks),\n",
        "                'emotion_score': f'{float(emo):.6f}',\n",
        "                'topic_id': int(t_id),\n",
        "                'topic_keywords': kw,\n",
        "            })\n",
        "\n",
        "print('Ready. Use run_pipeline(\'outputs/dreams_results.csv\', source=\'reddit\', limit=500)')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "run"},
      "source": [
        "# Voorbeeld-run (pas parameters aan naar wens)\n",
        "run_pipeline('outputs/dreams_results.csv', source='reddit', limit=500, k_min=5, k_max=15)\n",
        "print('CSV opgeslagen in outputs/dreams_results.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
